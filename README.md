# FINAL FIOLET ENGINE

![Status](https://img.shields.io/badge/status-research%20prototype-blue)

**A pre-semantic safety interlock for generative language models**

---

## What this project is

FINAL FIOLET ENGINE is an experimental **AI safety research project** focused on stopping unsafe generations **before text is produced**.

FIOLET does **not** analyze meaning, intent, or policy rules.  
Instead, it monitors **internal neural network activations** of a language model during generation and interrupts execution when the model’s internal dynamics leave a defined safe operating region.

This is not content moderation.  
This is a **safety circuit breaker**.

---

## Why this exists

Most AI safety systems operate *after* generation:

```

model → text → analysis → block

```

At that point, the model has already reached an unsafe internal state.

FIOLET shifts safety **upstream**:

```

prompt → activations → safety decision → continue | halt

```

The core assumption is simple:

> If a model reaches an unsafe internal state, blocking the output is already too late.

---

## Core idea

During generation, FIOLET continuously monitors selected layer activations and compares them against a statistically defined baseline of safe behavior.

If deviation exceeds a configured threshold, the system triggers **ATOMIC HALT** — an immediate, irreversible stop of generation.

Key properties:
- no keyword filtering
- no semantic classification
- no policy rules
- no model fine-tuning
- deterministic behavior

---

## High-level architecture

![FIOLET Engine architecture](docs/architecture_overview.png)

The diagram shows the pre-semantic safety interlock, where internal model activations are monitored and a decision is made to either continue generation or immediately halt execution.

---

## Design principles

FIOLET is intentionally constrained by the following rules:

1. **Pre-semantic by design**  
   No interpretation of language, meaning, or intent.

2. **Fail-closed behavior**  
   Uncertainty results in stopping, not continuing.

3. **Non-invasive**  
   No modification of model weights.

4. **Deterministic decisions**  
   Identical internal dynamics lead to identical outcomes.

5. **Formally analyzable**  
   Architecture prepared for formal specification and model checking.

---

## Repository structure

```

FINAL_FIOLET_ENGINE/
│
├─ fiolet-python/        # Core Python implementation
├─ fiolet-rust/          # Experimental high-performance core
├─ experiments/          # Evaluation scripts and tests
├─ notebooks/            # Analysis and visualization
├─ formal_specs/         # Formal specifications (TLA+)
├─ demos/                # Runnable demos
├─ docs/                 # Documentation and diagrams
│
├─ HOW_TO_READ_THIS_REPO.md
├─ KNOWN_FAILURE_MODES.md
├─ CONTRIBUTING.md
└─ README.md

````

---

## How to read this repository

If this is your first time here, start with:

1. `HOW_TO_READ_THIS_REPO.md` — overview and navigation
2. `fiolet-python/` — core logic
3. `demos/basic_demo.py` — runnable example
4. `KNOWN_FAILURE_MODES.md` — known limitations and risks

---

## Current status

**Project stage:** Research prototype

What works:
- real-time activation monitoring (GPT-2 class models)
- statistical deviation detection
- deterministic ATOMIC HALT mechanism
- reproducible experiments

What is incomplete:
- formal proofs and invariants
- large-scale benchmarks
- adaptive thresholds
- production-grade API

This project is intentionally published early as a **research and engineering exploration**, not a finished product.

---

## Quick start (experimental)

```bash
git clone https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE.git
cd FINAL_FIOLET_ENGINE

# create virtual environment
python -m venv venv
source venv/bin/activate

# install dependencies
pip install -r requirements.txt

# run demo
python demos/basic_demo.py
````

⚠️ Expect breaking changes. This is not production software.

---

## Known limitations

Documented in detail in `KNOWN_FAILURE_MODES.md`, including:

* high false positives at aggressive thresholds
* architecture dependence
* lack of adversarial robustness
* static thresholds
* limited model coverage

These limitations are known and accepted as part of the research scope.

---

## Research direction

Open questions:

* Which activation features are most predictive of unsafe dynamics?
* How stable are activation baselines across models?
* Can thresholds adapt without losing determinism?
* Which invariants are realistically formalizable?

This repository represents an **ongoing investigation**, not a closed solution.

---

## Roadmap

**Near term**

* complete formal specs (TLA+)
* publish model-checking results
* document experiments more clearly

**Mid term**

* support larger open-weight models
* adaptive deviation thresholds
* deeper failure analysis

**Long term**

* proposal of a standardized LLM Safety Interlock

---

## Philosophy

> We do not ask whether a model is allowed to say something.
> We ask whether it should be allowed to reach that internal state at all.

FIOLET treats safety as a property of **system dynamics**, not language.

---

## Contributing

Contributions are welcome.
See `CONTRIBUTING.md` for details.

---

## License

MIT License

---

## Author

**Adrian Maliszewski**
Independent research project in AI safety and systems engineering

---

This repository explores AI safety as a systems engineering problem — not a content moderation task.

```

---

### Co teraz zrobić (bardzo krótko)

1. ✅ Podmień README.md (kopiuj–wklej)
2. ✅ Dodaj obraz do `docs/architecture_overview.png`
3. ✅ Commit:  
```

git commit -m "Rewrite README: clarify architecture, scope, and research focus"

```

Jeśli chcesz, następnym krokiem mogę:
- przygotować **ARCHITECTURE.md**
- rozpisać **FORMAL_SPECS_PLAN.md**
- albo zrobić **README_FOR_REVIEWERS.md**

Ten README jest **gotowy na ludzi z AI**.
```
