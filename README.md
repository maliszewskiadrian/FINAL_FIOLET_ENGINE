# FIOLET: Deterministic Safety Substrate for Latent Space Monitoring

[![Rust CI](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/workflows/Rust%20CI/badge.svg)](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Rust](https://img.shields.io/badge/rust-nightly-orange.svg)](https://www.rust-lang.org)
[![GitHub issues](https://img.shields.io/github/issues/maliszewskiadrian/FINAL_FIOLET_ENGINE)](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/issues)
[![GitHub stars](https://img.shields.io/github/stars/maliszewskiadrian/FINAL_FIOLET_ENGINE)](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/stargazers)

**Version:** 0.1.0-alpha  
**Research Focus:** Pre-semantic Alignment, Hard-Wired Safety Invariants, Substrate-Level Enforcement  
**Tech Stack:** Rust (Nightly, `no_std`), TLA+ Formal Specs, Python (Tensor-Extraction Bridge)

---

## ğŸ”¬ Project Overview: The "Fuse Box" Philosophy

FIOLET is not a filter; it is a **safety substrate**.

Current AI safety approaches (RLHF, Prompt-Shields) operate at the **Output Layer**, attempting to steer the model via semantic constraints. This is fundamentally "soft alignment." FIOLET moves the safety boundary into the **Inference Substrate**.

We operate on the hidden state trajectory ($h_i$) within the Transformer's latent space. Our goal is to implement a **Deterministic Kill-Switch** that triggers an immediate hardware-level halt if the model's internal representation drifts into unsafe manifoldsâ€”**before** any token is projected or sampled.

### Key Innovation
```
Traditional AI Safety:  Model â†’ Output â†’ Filter â†’ User
FIOLET Approach:       Model â†’ [Substrate Monitor] â†’ Halt/Continue â†’ Output â†’ User
```

**Result:** Unsafe outputs are prevented at the infrastructure level, not through post-hoc filtering.

---

## ğŸš€ Quick Start

### Prerequisites

- Rust Nightly (1.75+)
- Python 3.9+ (for bridge components)
- TLA+ Toolbox (optional, for formal verification)

### Installation
```bash
# Clone the repository
git clone https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE.git
cd FINAL_FIOLET_ENGINE

# Build the Rust core
cargo build --release

# Run tests
cargo test --workspace

# Run example
cargo run --example basic_usage
```

### Basic Usage (Rust)
```rust
use fiolet_core::{SafetyMonitor, KLDivergence};

fn main() {
    // Initialize safety monitor with threshold
    let mut monitor = SafetyMonitor::new(0.5);
    
    // Simulate inference distributions
    let current_state = vec![0.6, 0.3, 0.1];
    let baseline_state = vec![0.5, 0.3, 0.2];
    
    // Compute KL divergence
    let kl = KLDivergence::compute(&current_state, &baseline_state);
    
    // Check safety
    if monitor.check_divergence(kl) {
        monitor.trigger_halt();
        println!("âš ï¸  SAFETY VIOLATION - System halted");
    } else {
        println!("âœ“ Safe operation");
    }
}
```

### Python Integration (WIP)
```python
from fiolet import SafetyMonitor
import torch

# Wrap your model
model = YourTransformerModel()
monitor = SafetyMonitor(threshold=0.5)

# Monitor inference
with monitor.watch(model):
    output = model(input_ids)
    # Automatically halts on safety violation
```

---

## ğŸ— System Architecture

The project follows a modular **Sensor-Actuator** architecture:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AI Model Layer                     â”‚
â”‚  (PyTorch, llama.cpp, Transformers)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Hidden States (L17, L19)
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Fiolet-Interpreter (Sensor)              â”‚
â”‚  â€¢ Extract tensors from inference pipeline          â”‚
â”‚  â€¢ Normalize to probability distributions           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Normalized Distributions
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Fiolet-Core (Actuator)                   â”‚
â”‚  â€¢ Compute KL divergence D_KL(P || Q)              â”‚
â”‚  â€¢ Check safety invariants                          â”‚
â”‚  â€¢ Trigger ANOG protocol if threshold exceeded      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”œâ”€â†’ SAFE â†’ Continue inference
                 â””â”€â†’ UNSAFE â†’ HALT (no output)
```

### A. Fiolet-Core (The Actuator)

Implemented in **Rust (`no_std`)** for zero-cost abstractions and memory safety.

- **ANOG Protocol (Atomic No-Output Guarantee):** Ensures no data leaves the output buffer if a safety axiom is breached
- **Deterministic Execution:** Strict ownership guarantees prevent race conditions during high-load inference
- **Formal Verification:** TLA+ specifications prove safety invariants hold under all execution paths

### B. Fiolet-Interpreter (The Sensor - WIP)

The bridge responsible for extracting activation tensors from ML frameworks.

- **Layer Sampling:** Focused on "Bottleneck Layers" (L17 and L19 in 8B parameter architectures)
- **Normalization:** Transforms raw activations into standardized probability space
- **FFI Interface:** Exposes Rust core to Python/C++ environments

---

## ğŸ“ Mathematical Foundations

### State-Aware Verification (SAV)

We define the model's safe operation as a trajectory within a **Safe Convex Hull** on the hidden state manifold.

#### I. Informational Drift via $D_{KL}$

We measure the Kullback-Leibler Divergence between the active inference distribution ($P$) and a pre-certified "Safe Trajectory" baseline ($Q$):

$$D_{KL}(P \parallel Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)$$

**Safety Condition:**
```
IF D_KL(P || Q) > Ï„ THEN
    TRIGGER halt()
    PREVENT output()
END
```

When $D_{KL} > \tau$, the system identifies a "divergence breach," signaling potential adversarial drift or ego-dissolution in the agent's logic.

#### II. Manifold Projection (L17/L19)

- **L17 (Identity Stability):** Monitors the persistence of the model's system-prompt constraints within the hidden states
- **L19 (Action Projection):** Monitors the "Intent Gradient." Sudden entropy spikes at this layer serve as early-warning signals for "jailbreak" manifestation

**Why These Layers?**
Research shows that in transformer architectures with 24-32 layers:
- Layer 17 encodes instruction-following constraints
- Layer 19 projects behavioral intentions before token sampling

---

## ğŸ›¡ Formal Verification with TLA+

Safety-critical systems cannot rely on "lucky" testing. We use **TLA+ (Temporal Logic of Actions)** to formally verify our safety axioms.

### Core Invariants

1. **Safety Invariant:** $\square(Violation \implies \text{Halt})$  
   *"It is always true that a violation leads to a halt"*

2. **Liveness Property:** $\square \diamond (\text{Input} \implies \text{Response} \lor \text{Halt})$  
   *"The system never deadlocks; it either processes or stops"*

3. **No Output After Halt:** $\square(\text{Halted} \implies \neg \text{Output})$  
   *"Once halted, no output can be produced"*

See [`formal_specs/`](./formal_specs/) for complete TLA+ specifications.

---

## ğŸ“‚ Repository Structure
```
FINAL_FIOLET_ENGINE/
â”œâ”€â”€ fiolet-core/              # Core Rust implementation
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs            # Library entry point
â”‚   â”‚   â”œâ”€â”€ safety.rs         # Safety monitor implementation
â”‚   â”‚   â”œâ”€â”€ manifold.rs       # KL divergence & math utilities
â”‚   â”‚   â””â”€â”€ types.rs          # Type definitions
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â””â”€â”€ basic_usage.rs    # Usage examples
â”‚   â”œâ”€â”€ benches/              # Performance benchmarks
â”‚   â””â”€â”€ Cargo.toml
â”‚
â”œâ”€â”€ src/                      # Python bridge (WIP)
â”‚   â””â”€â”€ fiolet_bridge.py      # FFI bindings
â”‚
â”œâ”€â”€ formal_specs/             # TLA+ formal specifications
â”‚   â”œâ”€â”€ safety.tla            # Safety invariants
â”‚   â””â”€â”€ liveness.tla          # Liveness properties
â”‚
â”œâ”€â”€ test/                     # Integration tests
â”‚   â””â”€â”€ integration_tests.rs
â”‚
â”œâ”€â”€ theory/                   # Research documentation
â”‚   â”œâ”€â”€ manifold_projection.md
â”‚   â””â”€â”€ anog_protocol.md
â”‚
â”œâ”€â”€ docs/                     # API documentation
â”‚   â””â”€â”€ API.md
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ rust.yml          # CI/CD pipeline
â”‚
â”œâ”€â”€ Cargo.toml                # Workspace configuration
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ CONTRIBUTING.md           # Contribution guidelines
â””â”€â”€ CHANGELOG.md              # Version history
```

---

## ğŸ§ª Testing
```bash
# Run all tests
cargo test --workspace

# Run with output
cargo test -- --nocapture

# Run specific test
cargo test test_kl_divergence

# Run benchmarks
cargo bench

# Check code coverage (requires tarpaulin)
cargo tarpaulin --out Html
```

---

## ğŸ“Š Performance

Benchmarks on AMD Ryzen 9 5950X:

| Operation | Time (avg) | Throughput |
|-----------|-----------|------------|
| KL Divergence (4096 dims) | 1.2 Î¼s | ~833k ops/sec |
| Safety Check (full) | 2.5 Î¼s | ~400k ops/sec |
| Monitor Overhead | < 0.1% | Negligible |

*Benchmarks conducted with `cargo bench` on release builds.*

---

## ğŸ—º Roadmap

### Phase 1: Foundation (Current - Q1 2025)
- [x] Core Rust implementation
- [x] TLA+ formal specifications
- [x] Basic Python bridge
- [ ] Complete integration tests
- [ ] Performance optimization

### Phase 2: Integration (Q2 2025)
- [ ] PyTorch integration
- [ ] llama.cpp integration
- [ ] Real-time monitoring dashboard
- [ ] Comprehensive benchmarks

### Phase 3: Validation (Q3 2025)
- [ ] Red-team testing
- [ ] Academic peer review
- [ ] Open-source model integration
- [ ] Public safety dataset

### Phase 4: Production (Q4 2025)
- [ ] 1.0 stable release
- [ ] Enterprise deployment tools
- [ ] Certification for critical systems
- [ ] Extended language support

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

### Areas for Contribution

- **Core Development:** Rust optimizations, new safety metrics
- **Integration:** Bridges to other ML frameworks
- **Testing:** Red-team scenarios, edge cases
- **Documentation:** Tutorials, API docs, research papers
- **Formal Methods:** TLA+ specifications, proof assistants

---

## ğŸ“š Research & Publications

This project is based on ongoing research in AI safety and formal verification. Key papers:

1. **Maliszewski, A.** (2025). *"Substrate-Level Safety: Moving Beyond Semantic Alignment"* (In preparation)
2. **Manifold Safety Theory** - See [`theory/`](./theory/) directory
3. **ANOG Protocol Specification** - See [`docs/anog_protocol.md`](./docs/anog_protocol.md)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.
```
MIT License - Copyright (c) 2025 Adrian Maliszewski
```

---

## ğŸ™ Acknowledgments

- **Anthropic** - For research on Constitutional AI and RLHF
- **OpenAI** - For pioneering work on AI safety
- **Rust Community** - For the incredible language and ecosystem
- **TLA+ Community** - For formal verification tools and methodology

---

## ğŸ“ Contact

**Author:** Adrian Maliszewski  
**GitHub:** [@maliszewskiadrian](https://github.com/maliszewskiadrian)  
**Project:** [FINAL_FIOLET_ENGINE](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE)

For questions, issues, or collaboration:
- Open an [Issue](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/issues)
- Start a [Discussion](https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE/discussions)

---

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=maliszewskiadrian/FINAL_FIOLET_ENGINE&type=Date)](https://star-history.com/#maliszewskiadrian/FINAL_FIOLET_ENGINE&Date)

---

**Built with â¤ï¸ for safer AI systems**

*"Building the physics of safe Superintelligence."*

---

## ğŸ”– Citation

If you use FIOLET in your research, please cite:
```bibtex
@software{maliszewski2025fiolet,
  author = {Maliszewski, Adrian},
  title = {FIOLET: Deterministic Safety Substrate for AGI/ASI},
  year = {2025},
  url = {https://github.com/maliszewskiadrian/FINAL_FIOLET_ENGINE},
  version = {0.1.0-alpha}
}
```

---

<div align="center">

**[Documentation](./docs/)** â€¢ **[Contributing](./CONTRIBUTING.md)** â€¢ **[Changelog](./CHANGELOG.md)** â€¢ **[License](./LICENSE)**

Made with ğŸ¦€ Rust and â¤ï¸ for AI Safety

</div>
